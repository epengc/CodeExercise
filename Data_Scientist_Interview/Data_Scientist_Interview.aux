\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Questions 1}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Outlier}{1}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Box plot of data from the Michelson–Morley experiment displaying four outliers in the middle column, as well as one outlier in the first column.}}{1}{figure.1}\protected@file@percent }
\newlabel{figure4}{{1}{1}{Box plot of data from the Michelson–Morley experiment displaying four outliers in the middle column, as well as one outlier in the first column}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}K-means}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Basic Idea}{1}{subsubsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces K-means algorithm. Training examples are shown as dots, and cluster centroids are shown as crosses. (a) Original dataset. (b) Random initial cluster centroids. (c-f) Illustration of running two iterations of k-means. In each iteration, we assign each training example to the closest cluster centroid (shown by "painting" the training examples the same color as the cluster centroid to which is assigned); then we move each cluster centroid to the mean of the points assigned to it. Images courtesy of Michael Jordan.}}{2}{figure.2}\protected@file@percent }
\newlabel{figure1}{{2}{2}{K-means algorithm. Training examples are shown as dots, and cluster centroids are shown as crosses. (a) Original dataset. (b) Random initial cluster centroids. (c-f) Illustration of running two iterations of k-means. In each iteration, we assign each training example to the closest cluster centroid (shown by "painting" the training examples the same color as the cluster centroid to which is assigned); then we move each cluster centroid to the mean of the points assigned to it. Images courtesy of Michael Jordan}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Algorithm}{2}{subsubsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Implementation}{2}{subsubsection.1.2.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}K-means Pseudo}{3}{lstlisting.1}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Should Stop Function Pseudo}{3}{lstlisting.2}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}Get Labels Function}{3}{lstlisting.3}\protected@file@percent }
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}Get Centroid Function}{3}{lstlisting.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}Intuition}{4}{subsubsection.1.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces KMeans in other dimensions. (left) K-means in 2d. (right) K-means in 3d. You have to imagine k-means in 4d.}}{4}{figure.3}\protected@file@percent }
\newlabel{figure3}{{3}{4}{KMeans in other dimensions. (left) K-means in 2d. (right) K-means in 3d. You have to imagine k-means in 4d}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Expectation Maximization}{4}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces K-Means algorithm is the EM algorithm applied to this Bayes Net.}}{4}{figure.4}\protected@file@percent }
\newlabel{figure2}{{4}{4}{K-Means algorithm is the EM algorithm applied to this Bayes Net}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Question 2}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Decision Tree}{5}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Random Forest}{5}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Question 3}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sensitivity and Specificity}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Question 4}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Question 5}{5}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Anomaly Detection}{6}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}K Nearest Neighbor}{6}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}XGBoost}{6}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Density-based spatial clustering of applications with noise (DBSCAN)}{6}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Question 6}{6}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Question 7}{6}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Question 8}{6}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Random Forest Regress-or}{6}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Question 9}{6}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Question 10}{6}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Question 12}{7}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12}Question 13}{7}{section.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Bagging Algorithm}{7}{subsection.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Question 14}{7}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Question 15}{7}{section.14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15}Question 16}{7}{section.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16}Question 17}{7}{section.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Data Curve}}{8}{figure.5}\protected@file@percent }
\newlabel{figure1}{{5}{8}{Data Curve}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17}Question 18}{8}{section.17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {18}Question 19}{8}{section.18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {19}Question 20}{8}{section.19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {20}Data Scientist Work Process}{9}{section.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces }}{9}{figure.6}\protected@file@percent }
\newlabel{figure6}{{6}{9}{}{figure.6}{}}
